{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    flip=False\n",
    "    \n",
    "    def __init__(self, path_x, path_y, batch_size=16):\n",
    "        self.path_x = path_x\n",
    "        self.path_y = path_y\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.x_files = os.listdir(self.path_x)\n",
    "        self.x_files.sort()\n",
    "        \n",
    "        self.y = np.loadtxt(os.path.join(self.path_y,\"y\"))\n",
    "        \n",
    "        self.pop_list = np.arange(self.y.shape[0])\n",
    "          \n",
    "    def __next__(self):\n",
    "        selected = np.random.choice(self.pop_list, replace=False, size = self.batch_size)\n",
    "        \n",
    "        x = []\n",
    "        for c,select in enumerate(selected):\n",
    "            tmp = np.load(os.path.join(self.path_x, str(select).zfill(5)+\".npy\"))\n",
    "            x.append(tmp)\n",
    "        x = np.array(x)\n",
    "        \n",
    "        if self.flip:\n",
    "            x = np.rollaxis(x,3,1)\n",
    "            \n",
    "        x = x.astype(np.float32)\n",
    "        x /= 255\n",
    "            \n",
    "        return x, self.y[selected]\n",
    "        \n",
    "\n",
    "batch_size = 16\n",
    "num_steps = 10000000\n",
    "\n",
    "path_x = \"cifar/cifar_npy\"\n",
    "path_y = \"cifar/cifar_labels/\"\n",
    "generator = Generator(path_x = path_x,\n",
    "                      path_y = path_y,\n",
    "                      batch_size = batch_size)\n",
    "\n",
    "generator.flip=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     6
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G(\n",
       "  (convT_1): ConvTranspose2d (256, 256, kernel_size=(8, 8), stride=(1, 1), bias=False)\n",
       "  (bn_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (convT_2): ConvTranspose2d (256, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1))\n",
       "  (convT_3): ConvTranspose2d (128, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), output_padding=(1, 1), bias=False)\n",
       "  (bn_3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (conv1): Conv2d (64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        self.dr_rate = .2\n",
    "        \n",
    "        self.convT_1 = nn.ConvTranspose2d(in_channels=256,\n",
    "                                            out_channels=256,\n",
    "                                            kernel_size=(8,8),\n",
    "                                            padding=0,\n",
    "                                            output_padding=0,\n",
    "                                            stride=1,\n",
    "                                            bias=False)\n",
    "        self.bn_1 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        self.convT_2 = nn.ConvTranspose2d(in_channels=256,\n",
    "                                            out_channels=128,\n",
    "                                            kernel_size=5,\n",
    "                                            padding=2,\n",
    "                                            output_padding=1,\n",
    "                                            stride=2,\n",
    "                                            bias=True)\n",
    "        \n",
    "        self.convT_3 = nn.ConvTranspose2d(in_channels=128,\n",
    "                                            out_channels=64,\n",
    "                                            kernel_size=5,\n",
    "                                            padding=2,\n",
    "                                            output_padding=1,\n",
    "                                            stride=2,\n",
    "                                            bias=False)\n",
    "        self.bn_3 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=64,\n",
    "                                out_channels=3,\n",
    "                                kernel_size=3,\n",
    "                                padding=1,\n",
    "                                stride=1,\n",
    "                                bias=True)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        dropout = nn.Dropout(p=self.dr_rate)\n",
    "        z = dropout(F.leaky_relu(self.convT_1(z), negative_slope=0.1))\n",
    "        z = self.bn_1(z)\n",
    "        \n",
    "        z = dropout(F.leaky_relu(self.convT_2(z), negative_slope=0.1))\n",
    "        \n",
    "        z = F.leaky_relu(self.convT_3(z), negative_slope=0.1)\n",
    "        z = self.bn_3(z)\n",
    "        \n",
    "        z = F.sigmoid(self.conv1(z))   \n",
    "        return z\n",
    "        \n",
    "g = G()\n",
    "g.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D(\n",
       "  (conv1): Conv2d (3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv2): Conv2d (32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (conv3): Conv2d (64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "  (fc1): Linear(in_features=2048, out_features=512)\n",
       "  (fc2): Linear(in_features=512, out_features=1)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.dr_rate = 0.2\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3,\n",
    "                               out_channels=32, \n",
    "                               kernel_size=3, \n",
    "                               stride=2,\n",
    "                               padding=1)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32,\n",
    "                               out_channels=64, \n",
    "                               kernel_size=3, \n",
    "                               stride=2,\n",
    "                               padding=1)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64,\n",
    "                               out_channels=128, \n",
    "                               kernel_size=3, \n",
    "                               stride=2,\n",
    "                               padding=1)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=128 * 4 * 4, \n",
    "                             out_features=512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(in_features=512,\n",
    "                             out_features=1)\n",
    "\n",
    "    \n",
    "    def dr_rate(self, dr_rate):\n",
    "        self.dr_rate = dr_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        dropout = nn.Dropout(p=self.dr_rate)\n",
    "        # Convolutional Part\n",
    "        x = dropout(F.leaky_relu(self.conv1(x), negative_slope=.1))\n",
    "        x = dropout(F.leaky_relu(self.conv2(x), negative_slope=.1))\n",
    "        x = dropout(F.leaky_relu(self.conv3(x), negative_slope=.1))\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        # Dense Layers\n",
    "        x = dropout(F.relu(self.fc1(x)))\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "d = D()\n",
    "d.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_size = 256\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = optim.Adam(d.parameters(), lr=0.0001)\n",
    "g_optimizer = optim.Adam(g.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/10000000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/10000000 [00:00<1578:06:13,  1.76it/s]\u001b[A\n",
      "  0%|          | 6/10000000 [00:00<314:07:17,  8.84it/s] \u001b[A\n",
      "  0%|          | 11/10000000 [00:00<198:54:12, 13.97it/s]\u001b[A\n",
      "Exception in thread Thread-8:\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/fs2/home/alook/anaconda2/envs/torch/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/mnt/fs2/home/alook/anaconda2/envs/torch/lib/python3.6/site-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/mnt/fs2/home/alook/anaconda2/envs/torch/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "  3%|▎         | 336106/10000000 [1:59:26<57:14:08, 46.90it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b4769c97a6b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0md_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/torch/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         return F.binary_cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 372\u001b[0;31m                                       size_average=self.size_average)\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/torch/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m   1177\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for c in tqdm(range(num_steps)):\n",
    "    d.zero_grad()\n",
    "\n",
    "    # Train d on real\n",
    "    x_train, _ = next(generator)\n",
    "    x_train = Variable(torch.from_numpy(x_train).cuda())\n",
    "    y_train = torch.ones(batch_size, 1)\n",
    "    y_train = Variable(y_train.cuda())\n",
    "\n",
    "    y_pred = d(x_train)\n",
    "    d_loss = criterion(y_pred, y_train)\n",
    "    d_loss.backward()\n",
    "\n",
    "\n",
    "     # Train d on fake\n",
    "    z = torch.Tensor(batch_size, latent_size, 1, 1).normal_(0, 1)\n",
    "    z = Variable(z.cuda(), requires_grad=False)\n",
    "    y_train = torch.zeros(batch_size, 1)\n",
    "    y_train = Variable(y_train.cuda())\n",
    "\n",
    "    x_train = g(z).detach()\n",
    "    y_pred = d(x_train)\n",
    "    d_loss = criterion(y_pred, y_train)\n",
    "    d_loss.backward()\n",
    "\n",
    "    d_optimizer.step()\n",
    "\n",
    "    # Train g\n",
    "    g.zero_grad()\n",
    "\n",
    "    z = torch.Tensor(batch_size, latent_size, 1, 1).normal_(0, 1)\n",
    "    z = Variable(z.cuda(), requires_grad=False)\n",
    "    y_train = torch.ones(batch_size, 1)\n",
    "    y_train = Variable(y_train.cuda())\n",
    "\n",
    "    x_train = g(z)\n",
    "    y_pred = d(x_train)\n",
    "    g_loss = criterion(y_pred, y_train)\n",
    "    g_loss.backward()  \n",
    "\n",
    "    g_optimizer.step()\n",
    "    \n",
    "    if c%10000 ==0:\n",
    "        z = torch.Tensor(16, latent_size, 1, 1).normal_(0, 1)\n",
    "        z = Variable(z.cuda(), requires_grad=False)\n",
    "        y_train = torch.zeros(batch_size, 1)\n",
    "        y_train = Variable(y_train.cuda())\n",
    "\n",
    "        tmp = g(z).detach()\n",
    "        x = np.array(tmp.data)\n",
    "        x = np.rollaxis(x,1,4)\n",
    "\n",
    "\n",
    "        plt.figure(figsize = (4,4))\n",
    "        gs1 = gridspec.GridSpec(4, 4)\n",
    "        gs1.update(wspace=0.0, hspace=0.0) # set the spacing between axes. \n",
    "\n",
    "        for i in range(16):\n",
    "           # i = i + 1 # grid spec indexes from 0\n",
    "            ax1 = plt.subplot(gs1[i])\n",
    "\n",
    "            ax1.set_xticklabels([])\n",
    "            ax1.set_yticklabels([])\n",
    "            ax1.set_xticks([])\n",
    "            ax1.set_yticks([])\n",
    "            ax1.set_aspect('equal')\n",
    "            ax1.imshow(x[i])\n",
    "\n",
    "        plt.savefig(\"progress_dog/{}.png\".format(c), bbox_inches='tight', dpi=200)\n",
    "        plt.close()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(g.state_dict(),\"models/callback_g_dog\")\n",
    "torch.save(d.state_dict(),\"models/callback_d_dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
